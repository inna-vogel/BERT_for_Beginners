{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original code is from Jay Alammar https://github.com/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #1\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "     import subprocess, os, numpy as np\n",
    "     gpu_free_memories = [int(m) for m in subprocess.Popen(\"nvidia-smi --query-gpu=memory.free --format=csv,nounits,noheader\", \n",
    "stdout=subprocess.PIPE, \n",
    "shell=True).communicate()[0].strip().decode().split(\"\\n\")]\n",
    "     most_free_gpu = str(np.argmax(gpu_free_memories))\n",
    "     os.environ[\"CUDA_VISIBLE_DEVICES\"] = most_free_gpu\n",
    "     print(\"Using GPU #\" + most_free_gpu)\n",
    "except Exception as e:\n",
    "     print(\"Could not select GPU. Exception:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)\n",
      "\u001b[K    100% |████████████████████████████████| 450kB 3.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm (from transformers)\n",
      "  Downloading https://files.pythonhosted.org/packages/72/c9/7fc20feac72e79032a7c8138fd0d395dc6d8812b5b9edf53c3afd0b31017/tqdm-4.41.1-py2.py3-none-any.whl (56kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 12.9MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting requests (from transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl\n",
      "Collecting sacremoses (from transformers)\n",
      "  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
      "\u001b[K    100% |████████████████████████████████| 870kB 2.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sentencepiece (from transformers)\n",
      "  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.0MB 1.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting boto3 (from transformers)\n",
      "  Downloading https://files.pythonhosted.org/packages/00/34/75b2d38f0647cfbdfd00c62c1d3e4210f6c40fb8ff66a9a644c439e849ab/boto3-1.11.1-py2.py3-none-any.whl (128kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 11.5MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting numpy (from transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/62/20/4d43e141b5bc426ba38274933ef8e76e85c7adea2c321ecf9ebf7421cedf/numpy-1.18.1-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading https://files.pythonhosted.org/packages/63/ae/057469010c579f6f6f37fc9b63b2d70af4ec20f2a7e2f3a1c4f364b4dde0/regex-2020.1.8-cp36-cp36m-manylinux1_x86_64.whl (689kB)\n",
      "\u001b[K    100% |████████████████████████████████| 696kB 2.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Downloading https://files.pythonhosted.org/packages/b9/63/df50cac98ea0d5b006c55a399c3bf1db9da7b5a24de7890bc9cfd5dd9e99/certifi-2019.11.28-py2.py3-none-any.whl (156kB)\n",
      "\u001b[K    100% |████████████████████████████████| 163kB 9.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna<2.9,>=2.5 (from requests->transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests->transformers)\n",
      "  Downloading https://files.pythonhosted.org/packages/b4/40/a9837291310ee1ccc242ceb6ebfd9eb21539649f193a7c8c86ba15b98539/urllib3-1.25.7-py2.py3-none-any.whl (125kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 11.5MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting chardet<3.1.0,>=3.0.2 (from requests->transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl\n",
      "Collecting six (from sacremoses->transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/65/26/32b8464df2a97e6dd1b656ed26b2c194606c16fe163c695a992b36c11cdf/six-1.13.0-py2.py3-none-any.whl\n",
      "Collecting click (from sacremoses->transformers)\n",
      "  Downloading https://files.pythonhosted.org/packages/fa/37/45185cb5abbc30d7257104c434fe0b07e5a195a6847506c074527aa599ec/Click-7.0-py2.py3-none-any.whl (81kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 13.9MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting joblib (from sacremoses->transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl\n",
      "Collecting s3transfer<0.4.0,>=0.3.0 (from boto3->transformers)\n",
      "  Downloading https://files.pythonhosted.org/packages/d5/29/2a79d0223617d255eccac5fd9c147d65ee32f9aabe0e47ab75fd7568af24/s3transfer-0.3.0-py2.py3-none-any.whl (69kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 15.4MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting botocore<1.15.0,>=1.14.1 (from boto3->transformers)\n",
      "  Downloading https://files.pythonhosted.org/packages/f0/33/2b6a1689d98209ce987efa72ff61b40c3fc48260fcf5f7ff24d5c9ac9ff4/botocore-1.14.1-py2.py3-none-any.whl (5.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.9MB 299kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1 (from boto3->transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/83/94/7179c3832a6d45b266ddb2aac329e101367fbdb11f425f13771d27f225bb/jmespath-0.9.4-py2.py3-none-any.whl\n",
      "Collecting python-dateutil<3.0.0,>=2.1 (from botocore<1.15.0,>=1.14.1->boto3->transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl\n",
      "Collecting docutils<0.16,>=0.10 (from botocore<1.15.0,>=1.14.1->boto3->transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/22/cd/a6aa959dca619918ccb55023b4cb151949c64d4d5d55b3f4ffd7eee0c6e8/docutils-0.15.2-py3-none-any.whl\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Running setup.py bdist_wheel for sacremoses ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/vogel/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: tqdm, certifi, idna, urllib3, chardet, requests, regex, six, click, joblib, sacremoses, sentencepiece, python-dateutil, docutils, jmespath, botocore, s3transfer, boto3, numpy, transformers\n",
      "Successfully installed boto3-1.11.1 botocore-1.14.1 certifi-2019.11.28 chardet-3.0.4 click-7.0 docutils-0.15.2 idna-2.8 jmespath-0.9.4 joblib-0.14.1 numpy-1.18.1 python-dateutil-2.8.1 regex-2020.1.8 requests-2.22.0 s3transfer-0.3.0 sacremoses-0.0.38 sentencepiece-0.1.85 six-1.13.0 tqdm-4.41.1 transformers-2.3.0 urllib3-1.25.7\n"
     ]
    }
   ],
   "source": [
    "#install transformers\n",
    "\n",
    "!pip3 install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0114 15:44:17.353391 139692097914688 file_utils.py:35] PyTorch version 1.2.0 available.\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and inspect it (show how many examples are labeled as \"positive\" (1) and how many are labeled \"negative\" (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.tsv.txt', delimiter='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a stirring , funny and finally transporting re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apparently reassembled from the cutting room f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>they presume their audience wo n't sit still f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is a visually stunning rumination on love...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jonathan parker 's bartleby should have been t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6915</th>\n",
       "      <td>painful , horrifying and oppressively tragic ,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6916</th>\n",
       "      <td>take care is nicely performed by a quintet of ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6917</th>\n",
       "      <td>the script covers huge , heavy topics in a bla...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6918</th>\n",
       "      <td>a seriously bad film with seriously warped log...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6919</th>\n",
       "      <td>a deliciously nonsensical comedy about a city ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6920 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0  1\n",
       "0     a stirring , funny and finally transporting re...  1\n",
       "1     apparently reassembled from the cutting room f...  0\n",
       "2     they presume their audience wo n't sit still f...  0\n",
       "3     this is a visually stunning rumination on love...  1\n",
       "4     jonathan parker 's bartleby should have been t...  1\n",
       "...                                                 ... ..\n",
       "6915  painful , horrifying and oppressively tragic ,...  1\n",
       "6916  take care is nicely performed by a quintet of ...  0\n",
       "6917  the script covers huge , heavy topics in a bla...  0\n",
       "6918  a seriously bad film with seriously warped log...  0\n",
       "6919  a deliciously nonsensical comedy about a city ...  1\n",
       "\n",
       "[6920 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = df\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3610\n",
       "0    3310\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0114 15:54:40.537770 139692097914688 file_utils.py:362] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpy20fhnuc\n",
      "I0114 15:54:41.183779 139692097914688 file_utils.py:377] copying /tmp/tmpy20fhnuc to cache at /home/vogel/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0114 15:54:41.184854 139692097914688 file_utils.py:381] creating metadata file for /home/vogel/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0114 15:54:41.185885 139692097914688 file_utils.py:390] removing temp file /tmp/tmpy20fhnuc\n",
      "I0114 15:54:41.186332 139692097914688 tokenization_utils.py:398] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/vogel/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0114 15:54:41.621958 139692097914688 file_utils.py:362] https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-config.json not found in cache or force_download set to True, downloading to /tmp/tmpqe950dmh\n",
      "I0114 15:54:42.038866 139692097914688 file_utils.py:377] copying /tmp/tmpqe950dmh to cache at /home/vogel/.cache/torch/transformers/a41e817d5c0743e29e86ff85edc8c257e61bc8d88e4271bb1b243b6e7614c633.1ccd1a11c9ff276830e114ea477ea2407100f4a3be7bdc45d37be9e37fa71c7e\n",
      "I0114 15:54:42.039998 139692097914688 file_utils.py:381] creating metadata file for /home/vogel/.cache/torch/transformers/a41e817d5c0743e29e86ff85edc8c257e61bc8d88e4271bb1b243b6e7614c633.1ccd1a11c9ff276830e114ea477ea2407100f4a3be7bdc45d37be9e37fa71c7e\n",
      "I0114 15:54:42.040772 139692097914688 file_utils.py:390] removing temp file /tmp/tmpqe950dmh\n",
      "I0114 15:54:42.041565 139692097914688 configuration_utils.py:185] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-config.json from cache at /home/vogel/.cache/torch/transformers/a41e817d5c0743e29e86ff85edc8c257e61bc8d88e4271bb1b243b6e7614c633.1ccd1a11c9ff276830e114ea477ea2407100f4a3be7bdc45d37be9e37fa71c7e\n",
      "I0114 15:54:42.042277 139692097914688 configuration_utils.py:199] Model config {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0114 15:54:42.417526 139692097914688 file_utils.py:362] https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmpqvwp31s0\n",
      "I0114 15:54:52.441015 139692097914688 file_utils.py:377] copying /tmp/tmpqvwp31s0 to cache at /home/vogel/.cache/torch/transformers/7b8a8f0b21c4e7f6962451c9370a5d9af90372a5f64637a251f2de154d0fc72c.c2015533705b9dff680ae707e205a35e2860e8d148b45d35085419d74fe57ac5\n",
      "I0114 15:54:52.625223 139692097914688 file_utils.py:381] creating metadata file for /home/vogel/.cache/torch/transformers/7b8a8f0b21c4e7f6962451c9370a5d9af90372a5f64637a251f2de154d0fc72c.c2015533705b9dff680ae707e205a35e2860e8d148b45d35085419d74fe57ac5\n",
      "I0114 15:54:52.625986 139692097914688 file_utils.py:390] removing temp file /tmp/tmpqvwp31s0\n",
      "I0114 15:54:52.641137 139692097914688 modeling_utils.py:406] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-pytorch_model.bin from cache at /home/vogel/.cache/torch/transformers/7b8a8f0b21c4e7f6962451c9370a5d9af90372a5f64637a251f2de154d0fc72c.c2015533705b9dff680ae707e205a35e2860e8d148b45d35085419d74fe57ac5\n"
     ]
    }
   ],
   "source": [
    "# For BERT change to (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "\n",
    "# Load pretrained tokenizer and model\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the text data\n",
    "\n",
    "BERT breaks the input into words and subwords. Then special tokens for sentence classifications at the beginning of the text ([CLS]) and at the end of the sentence ([SEP]) are added. In the last step the tokenizer replaces the tokens with embedding ids.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = train_data[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That's what the input text looks like after tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [101, 1037, 18385, 1010, 6057, 1998, 2633, 182...\n",
       "1       [101, 4593, 2128, 27241, 23931, 2013, 1996, 62...\n",
       "2       [101, 2027, 3653, 23545, 2037, 4378, 24185, 10...\n",
       "3       [101, 2023, 2003, 1037, 17453, 14726, 19379, 1...\n",
       "4       [101, 5655, 6262, 1005, 1055, 12075, 2571, 376...\n",
       "                              ...                        \n",
       "6915    [101, 9145, 1010, 7570, 18752, 14116, 1998, 28...\n",
       "6916    [101, 2202, 2729, 2003, 19957, 2864, 2011, 103...\n",
       "6917    [101, 1996, 5896, 4472, 4121, 1010, 3082, 7832...\n",
       "6918    [101, 1037, 5667, 2919, 2143, 2007, 5667, 2561...\n",
       "6919    [101, 1037, 12090, 2135, 2512, 5054, 19570, 23...\n",
       "Name: 0, Length: 6920, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding all sentences to the same length with the token id 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  101,  1037, 18385, ...,     0,     0,     0],\n",
       "       [  101,  4593,  2128, ...,     0,     0,     0],\n",
       "       [  101,  2027,  3653, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  101,  1996,  5896, ...,     0,     0,     0],\n",
       "       [  101,  1037,  5667, ...,     0,     0,     0],\n",
       "       [  101,  1037, 12090, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 0\n",
    "# find longest text\n",
    "for i in tokenized_text.values:\n",
    "    #print(i)\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "#if sentence is shorter than the longest sentence, padd zeros \n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized_text.values])\n",
    "padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The sentences are now 2-d arrays with the following dimensions: tokens and number of input examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6920, 67)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(padded).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `attention_mask`: It's a mask to be used if the input sequence length is smaller than the max input sequence length in the current batch. It's the mask that we typically use for attention when a batch has varying length sentences.\n",
    " \n",
    "\n",
    "### The “Attention Mask” is simply an array of 1s (tokens) and 0s (padded zeros) indicating which tokens are padding and which aren’t \n",
    "\n",
    "[Attention Mask](https://mccormickml.com/2019/07/22/BERT-fine-tuning/#sentence-length--attention-mask)\n",
    "<img src=\"masking.PNG\" height=10 width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " ...\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]]\n",
      "(6920, 67)\n"
     ]
    }
   ],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "print(attention_mask)\n",
    "print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model() function runs the sentences through BERT\n",
    "\n",
    "### last_hidden_states are the outputs of DistilBERT. \n",
    "The output of DistilBert is a 3-d tuple with the shape / dimensions --> number of examples (6920), max number of tokens in the sequence (67), number of hidden units in the DistilBERT model (768). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(padded)  \n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The output is a vector for each input token. And each vector contains 768 numbers (hidden units in the DistilBERT model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.2159, -0.1403,  0.0083,  ..., -0.1369,  0.5867,  0.2011],\n",
       "          [-0.2471,  0.2468,  0.1008,  ..., -0.1631,  0.9349, -0.0715],\n",
       "          [ 0.0558,  0.3573,  0.4140,  ..., -0.2430,  0.1770, -0.5080],\n",
       "          ...,\n",
       "          [ 0.1864,  0.0193,  0.1864,  ..., -0.2175,  0.1604, -0.4050],\n",
       "          [-0.1004,  0.0651,  0.1240,  ..., -0.1649,  0.3568,  0.1218],\n",
       "          [-0.0114,  0.3297,  0.2317,  ..., -0.2362,  0.4217,  0.0895]],\n",
       " \n",
       "         [[-0.1726, -0.1448,  0.0022,  ..., -0.1744,  0.2139,  0.3720],\n",
       "          [ 0.0022,  0.1684,  0.1269,  ..., -0.1888, -0.0195, -0.0283],\n",
       "          [ 0.0257, -0.2458,  0.0717,  ..., -0.4339,  0.1622,  0.0133],\n",
       "          ...,\n",
       "          [ 0.0466,  0.0850,  0.1801,  ..., -0.0279,  0.1878,  0.4022],\n",
       "          [-0.2325,  0.0746,  0.1298,  ..., -0.1292,  0.0904,  0.3647],\n",
       "          [-0.0655, -0.2214,  0.1827,  ..., -0.1624,  0.1421,  0.0963]],\n",
       " \n",
       "         [[-0.0506,  0.0720, -0.0296,  ..., -0.0715,  0.7185,  0.2623],\n",
       "          [ 0.0536,  0.3136, -0.0598,  ...,  0.2676,  0.8668, -0.3380],\n",
       "          [ 0.3792,  0.2792,  0.0237,  ...,  0.0343,  0.4272,  0.1680],\n",
       "          ...,\n",
       "          [ 0.2295,  0.0179,  0.1896,  ..., -0.0725,  0.1776,  0.0065],\n",
       "          [ 0.4776,  0.0545,  0.0732,  ..., -0.1118,  0.2219, -0.0038],\n",
       "          [ 0.2757,  0.1092,  0.0957,  ..., -0.1413,  0.1903,  0.0480]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.0655, -0.0518, -0.1409,  ..., -0.0645,  0.6022,  0.2135],\n",
       "          [-0.1388, -0.2643, -0.3730,  ...,  0.1416,  0.7465, -0.4062],\n",
       "          [ 0.4451, -0.0495, -0.2538,  ..., -0.1317,  0.0744, -0.2182],\n",
       "          ...,\n",
       "          [ 0.0121,  0.0664,  0.1355,  ..., -0.1351,  0.0596,  0.4312],\n",
       "          [ 0.2858,  0.1074, -0.0675,  ..., -0.2254,  0.4139,  0.1925],\n",
       "          [-0.0242,  0.2324,  0.1520,  ..., -0.2130,  0.2117,  0.3134]],\n",
       " \n",
       "         [[-0.0852, -0.0487, -0.0814,  ..., -0.1359,  0.3951,  0.2289],\n",
       "          [-0.2409, -0.1853, -0.2926,  ..., -0.1778,  0.8022,  0.1119],\n",
       "          [ 0.1641, -0.0146, -0.0866,  ..., -0.4101,  0.3737, -0.2818],\n",
       "          ...,\n",
       "          [ 0.0313, -0.0616, -0.0388,  ..., -0.1146,  0.2336, -0.1422],\n",
       "          [ 0.0923, -0.0839,  0.0963,  ..., -0.0719,  0.2545, -0.0018],\n",
       "          [ 0.0867,  0.1704,  0.3577,  ..., -0.0636,  0.0836, -0.1998]],\n",
       " \n",
       "         [[-0.2944, -0.0923, -0.0083,  ..., -0.0516,  0.4350,  0.2889],\n",
       "          [-0.3361,  0.1819, -0.1328,  ..., -0.1553,  0.8175,  0.2301],\n",
       "          [-0.0039, -0.1527,  0.2471,  ..., -0.3899,  0.3394,  0.0263],\n",
       "          ...,\n",
       "          [-0.1473, -0.0756,  0.1934,  ..., -0.0059, -0.0552, -0.2198],\n",
       "          [-0.0198, -0.2047,  0.1223,  ..., -0.2881,  0.0755, -0.1100],\n",
       "          [ 0.1322, -0.4463,  0.1558,  ..., -0.1565,  0.2923, -0.0541]]]),)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only the first position ([CLS]) is needed for classification (the 768 float numbers from DistilBERT which corresponds to the sentences in the dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = last_hidden_states[0][:,0,:].numpy() #[:,0,:] --> [:(all sentences),0 (only the first position CLS),: (all hidden units output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.21593462, -0.14028919,  0.00831093, ..., -0.13694865,\n",
       "         0.58670056,  0.20112726],\n",
       "       [-0.17262723, -0.14476144,  0.00223403, ..., -0.17442508,\n",
       "         0.21386462,  0.37197468],\n",
       "       [-0.05063348,  0.07203963, -0.02959663, ..., -0.07148966,\n",
       "         0.71852344,  0.26225492],\n",
       "       ...,\n",
       "       [-0.06550992, -0.05184716, -0.14094445, ..., -0.06450661,\n",
       "         0.60223   ,  0.21347886],\n",
       "       [-0.08523144, -0.04869819, -0.08137506, ..., -0.13589351,\n",
       "         0.39505625,  0.22889729],\n",
       "       [-0.29436848, -0.0923472 , -0.00831686, ..., -0.05159127,\n",
       "         0.43497843,  0.28891596]], dtype=float32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model\n",
    "\n",
    "## Train (75%) / Test Split (25% of data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.816"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf.score(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out your own examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos prediction: [0]\n"
     ]
    }
   ],
   "source": [
    "#pos = [\"Joker is the best movie of our times worth watching This movie earns the audience applause\"]\n",
    "pos = [\"Joker is the worse movie of our times so not worth watching This movie is so bad\"]\n",
    "\n",
    "df_sent = pd.DataFrame(pos)\n",
    "tokenize_sent = df_sent[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "padding_pos = np.array([i + [0]*(max_len-len(i)) for i in tokenize_sent])\n",
    "attention = np.where(padding_pos != 0, 1, 0)\n",
    "\n",
    "np.array(padding_pos).shape\n",
    "input_id = torch.tensor(padding_pos)  \n",
    "attention = torch.tensor(attention)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_state = model(input_id, attention_mask=attention)\n",
    "\n",
    "feature = last_hidden_state[0][:,0,:].numpy()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Pos prediction: {}\". format(lr_clf.predict(feature)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
