{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with BERT for Beginners\n",
    "\n",
    "# The original code is from Jay Alammar \n",
    "\n",
    "https://github.com/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internal Enabling GPU (ignore it) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #0\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "     import subprocess, os, numpy as np\n",
    "     gpu_free_memories = [int(m) for m in subprocess.Popen(\"nvidia-smi --query-gpu=memory.free --format=csv,nounits,noheader\", \n",
    "stdout=subprocess.PIPE, \n",
    "shell=True).communicate()[0].strip().decode().split(\"\\n\")]\n",
    "     most_free_gpu = str(np.argmax(gpu_free_memories))\n",
    "     os.environ[\"CUDA_VISIBLE_DEVICES\"] = most_free_gpu\n",
    "     print(\"Using GPU #\" + most_free_gpu)\n",
    "except Exception as e:\n",
    "     print(\"Could not select GPU. Exception:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/63/ae/057469010c579f6f6f37fc9b63b2d70af4ec20f2a7e2f3a1c4f364b4dde0/regex-2020.1.8-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting requests (from transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl\n",
      "Collecting sentencepiece (from transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting tqdm (from transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/72/c9/7fc20feac72e79032a7c8138fd0d395dc6d8812b5b9edf53c3afd0b31017/tqdm-4.41.1-py2.py3-none-any.whl\n",
      "Collecting numpy (from transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/62/20/4d43e141b5bc426ba38274933ef8e76e85c7adea2c321ecf9ebf7421cedf/numpy-1.18.1-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting boto3 (from transformers)\n",
      "  Downloading https://files.pythonhosted.org/packages/8c/38/ed4b123d32b1dc7c8a86b7bbae6317b25119d1df4a659dfa63473498f929/boto3-1.11.2-py2.py3-none-any.whl (128kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 8.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sacremoses (from transformers)\n",
      "Collecting idna<2.9,>=2.5 (from requests->transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl\n",
      "Collecting chardet<3.1.0,>=3.0.2 (from requests->transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests->transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/b4/40/a9837291310ee1ccc242ceb6ebfd9eb21539649f193a7c8c86ba15b98539/urllib3-1.25.7-py2.py3-none-any.whl\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/b9/63/df50cac98ea0d5b006c55a399c3bf1db9da7b5a24de7890bc9cfd5dd9e99/certifi-2019.11.28-py2.py3-none-any.whl\n",
      "Collecting botocore<1.15.0,>=1.14.2 (from boto3->transformers)\n",
      "  Downloading https://files.pythonhosted.org/packages/7c/df/8dbb896b70968a32f5e8b1153b9756882db01c6ca1a92243d7b1afe00d12/botocore-1.14.2-py2.py3-none-any.whl (5.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.9MB 320kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0 (from boto3->transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/d5/29/2a79d0223617d255eccac5fd9c147d65ee32f9aabe0e47ab75fd7568af24/s3transfer-0.3.0-py2.py3-none-any.whl\n",
      "Collecting jmespath<1.0.0,>=0.7.1 (from boto3->transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/83/94/7179c3832a6d45b266ddb2aac329e101367fbdb11f425f13771d27f225bb/jmespath-0.9.4-py2.py3-none-any.whl\n",
      "Collecting joblib (from sacremoses->transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl\n",
      "Collecting click (from sacremoses->transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/fa/37/45185cb5abbc30d7257104c434fe0b07e5a195a6847506c074527aa599ec/Click-7.0-py2.py3-none-any.whl\n",
      "Collecting six (from sacremoses->transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/65/26/32b8464df2a97e6dd1b656ed26b2c194606c16fe163c695a992b36c11cdf/six-1.13.0-py2.py3-none-any.whl\n",
      "Collecting python-dateutil<3.0.0,>=2.1 (from botocore<1.15.0,>=1.14.2->boto3->transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl\n",
      "Collecting docutils<0.16,>=0.10 (from botocore<1.15.0,>=1.14.2->boto3->transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/22/cd/a6aa959dca619918ccb55023b4cb151949c64d4d5d55b3f4ffd7eee0c6e8/docutils-0.15.2-py3-none-any.whl\n",
      "Installing collected packages: regex, idna, chardet, urllib3, certifi, requests, sentencepiece, tqdm, numpy, jmespath, six, python-dateutil, docutils, botocore, s3transfer, boto3, joblib, click, sacremoses, transformers\n",
      "Successfully installed boto3-1.11.2 botocore-1.14.2 certifi-2019.11.28 chardet-3.0.4 click-7.0 docutils-0.15.2 idna-2.8 jmespath-0.9.4 joblib-0.14.1 numpy-1.18.1 python-dateutil-2.8.1 regex-2020.1.8 requests-2.22.0 s3transfer-0.3.0 sacremoses-0.0.38 sentencepiece-0.1.85 six-1.13.0 tqdm-4.41.1 transformers-2.3.0 urllib3-1.25.7\n"
     ]
    }
   ],
   "source": [
    "#install transformers\n",
    "\n",
    "!pip3 install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0115 09:51:55.027056 139667892651840 file_utils.py:35] PyTorch version 1.2.0 available.\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and inspect it (show how many examples are labeled as \"positive\" (1) and how many are labeled \"negative\" (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.tsv.txt', delimiter='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a stirring , funny and finally transporting re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apparently reassembled from the cutting room f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>they presume their audience wo n't sit still f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is a visually stunning rumination on love...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jonathan parker 's bartleby should have been t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6915</th>\n",
       "      <td>painful , horrifying and oppressively tragic ,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6916</th>\n",
       "      <td>take care is nicely performed by a quintet of ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6917</th>\n",
       "      <td>the script covers huge , heavy topics in a bla...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6918</th>\n",
       "      <td>a seriously bad film with seriously warped log...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6919</th>\n",
       "      <td>a deliciously nonsensical comedy about a city ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6920 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0  1\n",
       "0     a stirring , funny and finally transporting re...  1\n",
       "1     apparently reassembled from the cutting room f...  0\n",
       "2     they presume their audience wo n't sit still f...  0\n",
       "3     this is a visually stunning rumination on love...  1\n",
       "4     jonathan parker 's bartleby should have been t...  1\n",
       "...                                                 ... ..\n",
       "6915  painful , horrifying and oppressively tragic ,...  1\n",
       "6916  take care is nicely performed by a quintet of ...  0\n",
       "6917  the script covers huge , heavy topics in a bla...  0\n",
       "6918  a seriously bad film with seriously warped log...  0\n",
       "6919  a deliciously nonsensical comedy about a city ...  1\n",
       "\n",
       "[6920 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = df\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3610\n",
       "0    3310\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0115 09:51:56.273807 139667892651840 tokenization_utils.py:398] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/vogel/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0115 09:51:56.977018 139667892651840 configuration_utils.py:185] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-config.json from cache at /home/vogel/.cache/torch/transformers/a41e817d5c0743e29e86ff85edc8c257e61bc8d88e4271bb1b243b6e7614c633.1ccd1a11c9ff276830e114ea477ea2407100f4a3be7bdc45d37be9e37fa71c7e\n",
      "I0115 09:51:56.979455 139667892651840 configuration_utils.py:199] Model config {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0115 09:51:57.380239 139667892651840 modeling_utils.py:406] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-pytorch_model.bin from cache at /home/vogel/.cache/torch/transformers/7b8a8f0b21c4e7f6962451c9370a5d9af90372a5f64637a251f2de154d0fc72c.c2015533705b9dff680ae707e205a35e2860e8d148b45d35085419d74fe57ac5\n"
     ]
    }
   ],
   "source": [
    "# For BERT change to (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "\n",
    "# Load pretrained tokenizer and model\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the text data\n",
    "\n",
    "BERT breaks the input into words and subwords. Then special tokens for sentence classifications at the beginning of the text ([CLS]) and at the end of the sentence ([SEP]) are added. In the last step the tokenizer replaces the tokens with embedding ids.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = train_data[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That's what the input text looks like after tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [101, 1037, 18385, 1010, 6057, 1998, 2633, 182...\n",
       "1       [101, 4593, 2128, 27241, 23931, 2013, 1996, 62...\n",
       "2       [101, 2027, 3653, 23545, 2037, 4378, 24185, 10...\n",
       "3       [101, 2023, 2003, 1037, 17453, 14726, 19379, 1...\n",
       "4       [101, 5655, 6262, 1005, 1055, 12075, 2571, 376...\n",
       "                              ...                        \n",
       "6915    [101, 9145, 1010, 7570, 18752, 14116, 1998, 28...\n",
       "6916    [101, 2202, 2729, 2003, 19957, 2864, 2011, 103...\n",
       "6917    [101, 1996, 5896, 4472, 4121, 1010, 3082, 7832...\n",
       "6918    [101, 1037, 5667, 2919, 2143, 2007, 5667, 2561...\n",
       "6919    [101, 1037, 12090, 2135, 2512, 5054, 19570, 23...\n",
       "Name: 0, Length: 6920, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding all sentences to the same length with the token id 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  101,  1037, 18385, ...,     0,     0,     0],\n",
       "       [  101,  4593,  2128, ...,     0,     0,     0],\n",
       "       [  101,  2027,  3653, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  101,  1996,  5896, ...,     0,     0,     0],\n",
       "       [  101,  1037,  5667, ...,     0,     0,     0],\n",
       "       [  101,  1037, 12090, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 0\n",
    "# find longest text\n",
    "for i in tokenized_text.values:\n",
    "    #print(i)\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "#if sentence is shorter than the longest sentence, padd zeros \n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized_text.values])\n",
    "padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The sentences are now 2-d arrays with the following dimensions: tokens and number of input examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6920, 67)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(padded).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `attention_mask`: It's a mask to be used if the input sequence length is smaller than the max input sequence length in the current batch. It's the mask that we typically use for attention when a batch has varying length sentences.\n",
    " \n",
    "\n",
    "### The “Attention Mask” is simply an array of 1s (tokens) and 0s (padded zeros) indicating which tokens are padding and which aren’t \n",
    "\n",
    "[Attention Mask](https://mccormickml.com/2019/07/22/BERT-fine-tuning/#sentence-length--attention-mask)\n",
    "<img src=\"masking.PNG\" height=10 width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " ...\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]]\n",
      "(6920, 67)\n"
     ]
    }
   ],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "print(attention_mask)\n",
    "print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model() function runs the sentences through BERT\n",
    "\n",
    "### last_hidden_states are the outputs of DistilBERT. \n",
    "The output of DistilBert is a 3-d tuple with the shape / dimensions --> number of examples (6920), max number of tokens in the sequence (67), number of hidden units in the DistilBERT model (768). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(padded)  \n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The output is a vector for each input token. And each vector contains 768 numbers (hidden units in the DistilBERT model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.2159, -0.1403,  0.0083,  ..., -0.1369,  0.5867,  0.2011],\n",
       "          [-0.2471,  0.2468,  0.1008,  ..., -0.1631,  0.9349, -0.0715],\n",
       "          [ 0.0558,  0.3573,  0.4140,  ..., -0.2430,  0.1770, -0.5080],\n",
       "          ...,\n",
       "          [ 0.1864,  0.0193,  0.1864,  ..., -0.2175,  0.1604, -0.4050],\n",
       "          [-0.1004,  0.0651,  0.1240,  ..., -0.1649,  0.3568,  0.1218],\n",
       "          [-0.0114,  0.3297,  0.2317,  ..., -0.2362,  0.4217,  0.0895]],\n",
       " \n",
       "         [[-0.1726, -0.1448,  0.0022,  ..., -0.1744,  0.2139,  0.3720],\n",
       "          [ 0.0022,  0.1684,  0.1269,  ..., -0.1888, -0.0195, -0.0283],\n",
       "          [ 0.0257, -0.2458,  0.0717,  ..., -0.4339,  0.1622,  0.0133],\n",
       "          ...,\n",
       "          [ 0.0466,  0.0850,  0.1801,  ..., -0.0279,  0.1878,  0.4022],\n",
       "          [-0.2325,  0.0746,  0.1298,  ..., -0.1292,  0.0904,  0.3647],\n",
       "          [-0.0655, -0.2214,  0.1827,  ..., -0.1624,  0.1421,  0.0963]],\n",
       " \n",
       "         [[-0.0506,  0.0720, -0.0296,  ..., -0.0715,  0.7185,  0.2623],\n",
       "          [ 0.0536,  0.3136, -0.0598,  ...,  0.2676,  0.8668, -0.3380],\n",
       "          [ 0.3792,  0.2792,  0.0237,  ...,  0.0343,  0.4272,  0.1680],\n",
       "          ...,\n",
       "          [ 0.2295,  0.0179,  0.1896,  ..., -0.0725,  0.1776,  0.0065],\n",
       "          [ 0.4776,  0.0545,  0.0732,  ..., -0.1118,  0.2219, -0.0038],\n",
       "          [ 0.2757,  0.1092,  0.0957,  ..., -0.1413,  0.1903,  0.0480]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.0655, -0.0518, -0.1409,  ..., -0.0645,  0.6022,  0.2135],\n",
       "          [-0.1388, -0.2643, -0.3730,  ...,  0.1416,  0.7465, -0.4062],\n",
       "          [ 0.4451, -0.0495, -0.2538,  ..., -0.1317,  0.0744, -0.2182],\n",
       "          ...,\n",
       "          [ 0.0121,  0.0664,  0.1355,  ..., -0.1351,  0.0596,  0.4312],\n",
       "          [ 0.2858,  0.1074, -0.0675,  ..., -0.2254,  0.4139,  0.1925],\n",
       "          [-0.0242,  0.2324,  0.1520,  ..., -0.2130,  0.2117,  0.3134]],\n",
       " \n",
       "         [[-0.0852, -0.0487, -0.0814,  ..., -0.1359,  0.3951,  0.2289],\n",
       "          [-0.2409, -0.1853, -0.2926,  ..., -0.1778,  0.8022,  0.1119],\n",
       "          [ 0.1641, -0.0146, -0.0866,  ..., -0.4101,  0.3737, -0.2818],\n",
       "          ...,\n",
       "          [ 0.0313, -0.0616, -0.0388,  ..., -0.1146,  0.2336, -0.1422],\n",
       "          [ 0.0923, -0.0839,  0.0963,  ..., -0.0719,  0.2545, -0.0018],\n",
       "          [ 0.0867,  0.1704,  0.3577,  ..., -0.0636,  0.0836, -0.1998]],\n",
       " \n",
       "         [[-0.2944, -0.0923, -0.0083,  ..., -0.0516,  0.4350,  0.2889],\n",
       "          [-0.3361,  0.1819, -0.1328,  ..., -0.1553,  0.8175,  0.2301],\n",
       "          [-0.0039, -0.1527,  0.2471,  ..., -0.3899,  0.3394,  0.0263],\n",
       "          ...,\n",
       "          [-0.1473, -0.0756,  0.1934,  ..., -0.0059, -0.0552, -0.2198],\n",
       "          [-0.0198, -0.2047,  0.1223,  ..., -0.2881,  0.0755, -0.1100],\n",
       "          [ 0.1322, -0.4463,  0.1558,  ..., -0.1565,  0.2923, -0.0541]]]),)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only the first position ([CLS]) is needed for classification (the 768 float numbers from DistilBERT which corresponds to the sentences in the dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = last_hidden_states[0][:,0,:].numpy() #[:,0,:] --> [:(all sentences),0 (only the first position CLS),: (all hidden units output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.21593462, -0.14028919,  0.00831093, ..., -0.13694865,\n",
       "         0.58670056,  0.20112726],\n",
       "       [-0.17262723, -0.14476144,  0.00223403, ..., -0.17442508,\n",
       "         0.21386462,  0.37197468],\n",
       "       [-0.05063348,  0.07203963, -0.02959663, ..., -0.07148966,\n",
       "         0.71852344,  0.26225492],\n",
       "       ...,\n",
       "       [-0.06550992, -0.05184716, -0.14094445, ..., -0.06450661,\n",
       "         0.60223   ,  0.21347886],\n",
       "       [-0.08523144, -0.04869819, -0.08137506, ..., -0.13589351,\n",
       "         0.39505625,  0.22889729],\n",
       "       [-0.29436848, -0.0923472 , -0.00831686, ..., -0.05159127,\n",
       "         0.43497843,  0.28891596]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model\n",
    "\n",
    "## Train (75%) / Test Split (25% of data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8445086705202313"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf.score(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out your own examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize input\n",
    "\n",
    "def vectorize_input(list_of_strings):\n",
    "    df_sent = pd.DataFrame(list_of_strings)\n",
    "    tokenize_sent = df_sent[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "    padding_pos = np.array([i + [0]*(max_len-len(i)) for i in tokenize_sent])\n",
    "    attention = np.where(padding_pos != 0, 1, 0)\n",
    "    input_id = torch.tensor(padding_pos)  \n",
    "    attention = torch.tensor(attention)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        last_hidden_state = model(input_id, attention_mask=attention)\n",
    "\n",
    "    feature = last_hidden_state[0][:,0,:].numpy()\n",
    "    return feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos prediction for the sentence ['Joker is the best movie of our times worth watching This movie earns the audience applause']:[1]\n"
     ]
    }
   ],
   "source": [
    "pos = [\"Joker is the best movie of our times worth watching This movie earns the audience applause\"]\n",
    "#pos = [\"Joker is the worse movie of our times so not worth watching This movie is so bad\"]\n",
    "\n",
    "vectorize_pos = vectorize_input(pos) \n",
    "\n",
    "print(\"Pos prediction for the sentence \" \"{}:\" \"{}\". format(pos, lr_clf.predict(vectorize_pos)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
